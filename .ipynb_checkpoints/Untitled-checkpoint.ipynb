{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Didi Competition ( round 1 )\n",
    "----------\n",
    "   ## car detection using LIDAR data only\n",
    "   \n",
    "\n",
    "The following animation shows the final results of vehicle prediction on test dataset by applying the U-net algorithm.\n",
    "\n",
    "![](output.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://github.com/omgteam/Didi-competition-solution) you can find more instructions about how to deal with visualization, caliberation, and detection ROS nodes.\n",
    "\n",
    "\n",
    "\n",
    "## LIDAR Visualization on ROS\n",
    "\n",
    "\n",
    "To visualize Dataset Release provided by Udacity, we need to convert ` topic /velodyne_packets to /velodyne_points`. We need to install ROS velodyne drivers [here](https://github.com/ros-drivers/velodyne.git), and then do the following instructions:\n",
    "\n",
    "  $ roslaunch velodyne_pointcloud 32e_points.launch\n",
    "  \n",
    "  $ roslaunch `didi_visualize display_rosbag_rviz.launch rosbag_file:=PATH/NAME.bag`\n",
    "\n",
    "\n",
    ">This module is borrowed from [here](https://github.com/jokla/didi_challenge_ros).\n",
    "\n",
    "You will get something looks like as same as what the following animation displayed when you play a rosbag file and visualize the result in RViz\n",
    "\n",
    "![](lidar.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Detection\n",
    "\n",
    "I used the small U-net for detecting vehicles by using lidar data only(the lidar data is provided by KITTI). U-net is a encoder-decoder type of network for pixel-wise predictions(image segmentation). What makes U-net unique is because it obtains a special shape of layer where the feature maps from one convolution part in `downsampling ` step are going to be fed to the up-convolution part in `up-sampling` step.\n",
    "\n",
    "\n",
    "The following figure could provide you with a better view of how U-net model works.\n",
    "![](./examples/U-net.png)\n",
    "\n",
    "\n",
    "The original research paper on U-net is [here](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "* KITTI Data\n",
    "        KITTI dataset is a freely available data of a car driving in urban environment. The KITTI car has 4 cameras (2 stereo color and 2 stereo grayscale), velodyneâ€™s VLP-64 LIDAR and an GPS/IMU. In addition calibration data are provided so transformations between velodyne (LIDAR), IMU and camera images can be made.\n",
    "        \n",
    "\n",
    "### Exploratory data analysis of KITTI dataset\n",
    "\n",
    "the LIDAR data output is a set of points x,y,z and reflective intensity from these points. We will use these points to make our front-view & top-view.\n",
    "\n",
    "|original image   | front  view   | top view       |\n",
    "|-----------------|:--------------|:---------------|\n",
    "|![](./examples/origin.png)|![](./examples/front.png)|![](./examples/top.png)|\n",
    "\n",
    "\n",
    "\n",
    "* top view of lidar data based on the original image\n",
    "\n",
    "|           original image                |           top view                   |\n",
    "|--------------------------|:-----------------------------|\n",
    "|![](./examples/kitti.png) | ![](./examples/top_lidar.png)|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the top-view showed above, I found it could show vehicles that are not even in the field of view of the front camera. Thus I decided to use only top-view lidar data to train the model and see how accurate it will be to detect vehilces. \n",
    "\n",
    "This snippet code shows the way of creating the birdseye view of lidar data is borrowed from [here](http://ronny.rest/tutorials/module/pointclouds_01/point_cloud_birdseye/) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def point_cloud_2_birdseye(points,\n",
    "                           res=0.1,\n",
    "                           side_range=(-10., 10.),  # left-most to right-most\n",
    "                           fwd_range = (-10., 10.), # back-most to forward-most\n",
    "                           height_range=(-2., 2.),  # bottom-most to upper-most\n",
    "                           ):\n",
    "    \"\"\" Creates an 2D birds eye view representation of the point cloud data.\n",
    "\n",
    "    Args:\n",
    "        points:     (numpy array)\n",
    "                    N rows of points data\n",
    "                    Each point should be specified by at least 3 elements x,y,z\n",
    "        res:        (float)\n",
    "                    Desired resolution in metres to use. Each output pixel will\n",
    "                    represent an square region res x res in size.\n",
    "        side_range: (tuple of two floats)\n",
    "                    (-left, right) in metres\n",
    "                    left and right limits of rectangle to look at.\n",
    "        fwd_range:  (tuple of two floats)\n",
    "                    (-behind, front) in metres\n",
    "                    back and front limits of rectangle to look at.\n",
    "        height_range: (tuple of two floats)\n",
    "                    (min, max) heights (in metres) relative to the origin.\n",
    "                    All height values will be clipped to this min and max value,\n",
    "                    such that anything below min will be truncated to min, and\n",
    "                    the same for values above max.\n",
    "    Returns:\n",
    "        2D numpy array representing an image of the birds eye view.\n",
    "    \"\"\"\n",
    "    # EXTRACT THE POINTS FOR EACH AXIS\n",
    "    x_points = points[:, 0]\n",
    "    y_points = points[:, 1]\n",
    "    z_points = points[:, 2]\n",
    "\n",
    "    # FILTER - To return only indices of points within desired cube\n",
    "    # Three filters for: Front-to-back, side-to-side, and height ranges\n",
    "    # Note left side is positive y axis in LIDAR coordinates\n",
    "    f_filt = np.logical_and((x_points > fwd_range[0]), (x_points < fwd_range[1]))\n",
    "    s_filt = np.logical_and((y_points > -side_range[1]), (y_points < -side_range[0]))\n",
    "    filter = np.logical_and(f_filt, s_filt)\n",
    "    indices = np.argwhere(filter).flatten()\n",
    "\n",
    "    # KEEPERS\n",
    "    x_points = x_points[indices]\n",
    "    y_points = y_points[indices]\n",
    "    z_points = z_points[indices]\n",
    "\n",
    "    # CONVERT TO PIXEL POSITION VALUES - Based on resolution\n",
    "    x_img = (-y_points / res).astype(np.int32)  # x axis is -y in LIDAR\n",
    "    y_img = (-x_points / res).astype(np.int32)  # y axis is -x in LIDAR\n",
    "\n",
    "    # SHIFT PIXELS TO HAVE MINIMUM BE (0,0)\n",
    "    # floor & ceil used to prevent anything being rounded to below 0 after shift\n",
    "    x_img -= int(np.floor(side_range[0] / res))\n",
    "    y_img += int(np.ceil(fwd_range[1] / res))\n",
    "\n",
    "    # CLIP HEIGHT VALUES - to between min and max heights\n",
    "    pixel_values = np.clip(a=z_points,\n",
    "                           a_min=height_range[0],\n",
    "                           a_max=height_range[1])\n",
    "\n",
    "    # RESCALE THE HEIGHT VALUES - to be between the range 0-255\n",
    "    pixel_values = scale_to_255(pixel_values,\n",
    "                                min=height_range[0],\n",
    "                                max=height_range[1])\n",
    "\n",
    "    # INITIALIZE EMPTY ARRAY - of the dimensions we want\n",
    "    x_max = 1 + int((side_range[1] - side_range[0]) / res)\n",
    "    y_max = 1 + int((fwd_range[1] - fwd_range[0]) / res)\n",
    "    im = np.zeros([y_max, x_max], dtype=np.uint8)\n",
    "\n",
    "    # FILL PIXEL VALUES IN IMAGE ARRAY\n",
    "    im[y_img, x_img] = pixel_values\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis of Didi dataset\n",
    "\n",
    "The big difference between kitti and didi dataset is that the LIDAR on KITTI is composed of an array of 64 laser units,however, Didi challenge dataset is collected using VLP-16 lidar.\n",
    "\n",
    "Here are two figures presented below show the results by using VLP-64 and VLP-16 lidar.\n",
    "\n",
    "\n",
    "|           VLP-64         |          VLP-16              |\n",
    "|--------------------------|:-----------------------------|\n",
    "|![](./examples/top_64.png) | ![](./examples/top_16.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Didi data Preprocessing and Augmentation\n",
    "\n",
    "* Translation\n",
    "\n",
    "I randomly shifted the birdseye image +-50 from the original.\n",
    "\n",
    "here is an example shows this step.\n",
    "\n",
    "![](./examples/trans.png)\n",
    "![](./examples/trans2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rotation\n",
    "\n",
    "I randomly rotated the birdseye image to +-90,+-180,+-270 from the original.\n",
    "\n",
    "here are examples show this step.\n",
    "\n",
    "![](./examples/rot1.png)\n",
    "![](./examples/rot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stretch\n",
    "\n",
    "I randomly enlarged/narrowed the vehicle in range of (+/-20).\n",
    "\n",
    "here is an example shows this step.\n",
    "\n",
    "** enlarge**\n",
    "![](./examples/strech1.png)\n",
    "\n",
    "** narrow**\n",
    "![](./examples/strech2.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't apply methods of  `saturation`, `brightness` and `contrast` to change the values of pixels as I found adding them would cause more useless noises that more cloud points were generated by doing so. Thus I escaped augmenting lidar data by using these functions.\n",
    "\n",
    "The following example images show the results after applying saturation.\n",
    "\n",
    "![](./examples/sat1.png)\n",
    "\n",
    "![](./examples/sat2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask\n",
    "Next we are going to apply the mask to take the segmentation of that vehicle out and leaves the rest as black, we then feed them to the U-net for trainning.\n",
    "\n",
    "here is an example shows this step.\n",
    "![](./examples/mask1.png)\n",
    "![](./examples/mask2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The input to U-net is a resized 480X480 image, the output is 480X480 1-channel mask of predictions. We then use an activation function of sigmoid on the last layer to predict how many pixels respresent in the place of where a vehicle will be. \n",
    "I chose a batch size of 1 for all architectures. This 1 image was randomly picked and augmented from all training images. Also I chose  the adam optimizer with a learning rate of 0.0001 for it. Training the segmentation model toke me about 24 minutes at each time.\n",
    "\n",
    "![](./examples/train_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "Surprisingly, with small number of training dataset this neural network could predict the car with higher accuracty. Figures below present result of segmentation algorithm applied for vehicle predictions. The panels are organized as original image, predicted mask and the corresponding predicted bounding boxes.\n",
    "![](./examples/output.png)\n",
    "\n",
    "\n",
    "\n",
    "Figures below present performance of the model for vehicle detection. It was surprising that the neural network was able to identify cars correctly in the test image. Figures below present result of segmentation algorithm applied for vehicle predictions. The panels are organized as original image, predicted mask and ground truth boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reflection\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
